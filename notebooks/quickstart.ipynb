{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9270986e",
   "metadata": {},
   "source": [
    "# üöÄ ResearchGPT Quickstart\n",
    "\n",
    "This notebook walks through the full pipeline on the sample paper:\n",
    "- Load PDF\n",
    "- Extract metadata\n",
    "- Clean & chunk text\n",
    "- Build index & run search\n",
    "- Summarize & analyze chunks\n",
    "- Save metadata JSON\n",
    "\n",
    "You can run each step interactively to understand the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5534a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root added: /home/isa/code/research_gpt_assistant\n",
      "sys.path[0]: /home/isa/code/research_gpt_assistant\n",
      "‚úÖ MISTRAL_API_KEY loaded? True\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Always ensure we're starting from project root\n",
    "project_root = Path.cwd().parent  # notebooks/ ‚Üí project root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"‚úÖ Project root added:\", project_root)\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "print(\"‚úÖ MISTRAL_API_KEY loaded?\", bool(os.getenv(\"MISTRAL_API_KEY\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d97eec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "from src.config import MISTRAL_API_KEY\n",
    "from src.pdf_utils import load_all_pdfs_text\n",
    "from src.text_utils import clean_text, chunk_text\n",
    "from src.indexer import build_index, search\n",
    "from src.summarizer import summarize_chunks\n",
    "from src.analyst import analyze_chunks\n",
    "from src.metadata_utils import extract_metadata\n",
    "from src.io_utils import safe_stem\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3868d2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for PDFs in: /home/isa/code/research_gpt_assistant/data/sample_papers\n",
      "Found PDFs: [PosixPath('/home/isa/code/research_gpt_assistant/data/sample_papers/attention_is_all_you_need.pdf')]\n"
     ]
    }
   ],
   "source": [
    "pdf_path = project_root / \"data/sample_papers/attention_is_all_you_need.pdf\"\n",
    "\n",
    "print(\"Looking for PDFs in:\", pdf_path.parent.resolve())\n",
    "pdfs = list(pdf_path.parent.glob(\"*.pdf\"))\n",
    "print(\"Found PDFs:\", pdfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b18b11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded PDF: /home/isa/code/research_gpt_assistant/data/sample_papers/attention_is_all_you_need.pdf\n",
      "First 500 characters:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez‚àó ‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu≈Åukasz Kaise\n"
     ]
    }
   ],
   "source": [
    "pairs = load_all_pdfs_text(pdf_path.parent)\n",
    "\n",
    "if not pairs:\n",
    "    raise FileNotFoundError(f\"No PDFs found in {pdf_path.parent.resolve()}\")\n",
    "\n",
    "pdf_path, raw_text = pairs[0]\n",
    "print(\"‚úÖ Loaded PDF:\", pdf_path)\n",
    "print(\"First 500 characters:\\n\", raw_text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f59ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted metadata:\n",
      "\n",
      "{\n",
      "  \"title\": \"Attention Is All You Need\",\n",
      "  \"authors\": null,\n",
      "  \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "meta = extract_metadata(pdf_path)\n",
    "print(\"‚úÖ Extracted metadata:\\n\")\n",
    "print(json.dumps(meta, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9412eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks: 30\n",
      "\n",
      "--- First 2 chunks ---\n",
      "\n",
      "Chunk 1:\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.comNiki Parma...\n",
      "\n",
      "Chunk 2:\n",
      "rench translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it s...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt = clean_text(raw_text)\n",
    "chunks = chunk_text(txt, max_chars=1500, overlap=150)\n",
    "\n",
    "print(f\"‚úÖ Total chunks: {len(chunks)}\")\n",
    "print(\"\\n--- First 2 chunks ---\\n\")\n",
    "for i, ch in enumerate(chunks[:2], 1):\n",
    "    print(f\"Chunk {i}:\\n{ch[:300]}...\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ddc8af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Top hits:\n",
      "\n",
      "- attention_is_all_you_need [chunk 14] (score 0.072)\n",
      " easier it is to learn long-range dependencies [ 12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As not...\n",
      "\n",
      "- attention_is_all_you_need [chunk 16] (score 0.043)\n",
      "e sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one ma...\n",
      "\n",
      "- attention_is_all_you_need [chunk 1] (score 0.041)\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = build_index([(f\"{pdf_path.stem} [chunk {i+1}]\", ch) for i, ch in enumerate(chunks)])\n",
    "hits = search(index, \"What problem does this paper solve?\", k=3)\n",
    "\n",
    "print(\"‚úÖ Top hits:\\n\")\n",
    "for score, (lbl, text) in hits:\n",
    "    print(f\"- {lbl} (score {score:.3f})\\n{text[:200]}...\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24e03c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary preview:\n",
      " - The paper introduces a new model architecture called the Transformer, which is based solely on attention mechanisms and does not use recurrence or convolutions.\n",
      "- The Transformer is faster than recurrent layers when the sequence length is smaller than the representation dimensionality, especially for sentence representations used in machine translations.\n",
      "- To improve computational performance for tasks involving very long sequences, the Transformer could be restricted to considering only a nei\n",
      "\n",
      "---\n",
      "\n",
      "‚úÖ Analysis preview:\n",
      " # Analysis: Attention Is All You Need\n",
      "\n",
      "\n",
      "## Methods\n",
      "\n",
      "- Methods:\n",
      "  - Transformer architecture, based solely on attention mechanisms, without recurrence or convolutions.\n",
      "  - Self-attention layers for connecting positions in the network, with computational complexity O(n) when considering all positions, or O(n/r) when considering a neighborhood of size r.\n",
      "  - Convolutional layers with kernel width k < n, requiring a stack of O(n/k) convolutional layers for connecting all pairs of input and output po\n"
     ]
    }
   ],
   "source": [
    "top_chunks = [text for _s, (_lbl, text) in hits]\n",
    "\n",
    "summary = summarize_chunks(MISTRAL_API_KEY, \"Attention Is All You Need\", top_chunks)\n",
    "analysis = analyze_chunks(MISTRAL_API_KEY, \"Attention Is All You Need\", top_chunks)\n",
    "\n",
    "print(\"‚úÖ Summary preview:\\n\", summary[:500])\n",
    "print(\"\\n---\\n\")\n",
    "print(\"‚úÖ Analysis preview:\\n\", analysis[:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "baa8d955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata object:\n",
      "\n",
      "{\n",
      "  \"file\": \"attention_is_all_you_need.pdf\",\n",
      "  \"title\": \"Attention Is All You Need\",\n",
      "  \"authors\": null,\n",
      "  \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023\",\n",
      "  \"query_used\": \"What problem does this paper solve?\",\n",
      "  \"outputs\": {\n",
      "    \"summary_preview\": \"- The paper introduces a new model architecture called the Transformer, which is based solely on attention mechanisms and does not use recurrence or convolutions.\\n- The Transformer is faster than recu...\",\n",
      "    \"analysis_preview\": \"# Analysis: Attention Is All You Need\\n\\n\\n## Methods\\n\\n- Methods:\\n  - Transformer architecture, based solely on attention mechanisms, without recurrence or convolutions.\\n  - Self-attention layers for con...\"\n",
      "  }\n",
      "}\n",
      "‚úÖ Saved to: /home/isa/code/research_gpt_assistant/results/metadata/attention_is_all_you_need_demo_meta.json\n"
     ]
    }
   ],
   "source": [
    "meta_out = {\n",
    "    \"file\": pdf_path.name,\n",
    "    \"title\": meta.get(\"title\", pdf_path.stem),\n",
    "    \"authors\": meta.get(\"authors\", \"Unknown\"),\n",
    "    \"abstract\": meta.get(\"abstract\"),\n",
    "    \"query_used\": \"What problem does this paper solve?\",\n",
    "    \"outputs\": {\n",
    "        \"summary_preview\": summary[:200] + \"...\",\n",
    "        \"analysis_preview\": analysis[:200] + \"...\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Metadata object:\\n\")\n",
    "print(json.dumps(meta_out, indent=2))\n",
    "\n",
    "out_path = project_root / \"results/metadata/attention_is_all_you_need_demo_meta.json\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_path.write_text(json.dumps(meta_out, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Saved to:\", out_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-gpt-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
