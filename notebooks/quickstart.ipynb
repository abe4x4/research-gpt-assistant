{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fd443a",
   "metadata": {},
   "source": [
    "# üöÄ ResearchGPT Quickstart\n",
    "\n",
    "This notebook walks through the full pipeline on the sample paper:\n",
    "- Load PDF\n",
    "- Extract metadata\n",
    "- Clean & chunk text\n",
    "- Build index & run search\n",
    "- Summarize & analyze chunks\n",
    "- Save metadata JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c272b4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: /home/isa/code/research_gpt_assistant\n",
      "‚úÖ MISTRAL_API_KEY loaded? True\n",
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1) Project root handling ---\n",
    "project_root = Path.cwd().parent  # from notebooks/ ‚Üí go up one level\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"‚úÖ Project root:\", project_root)\n",
    "\n",
    "# --- 2) Load environment variables ---\n",
    "load_dotenv()\n",
    "print(\"‚úÖ MISTRAL_API_KEY loaded?\", bool(os.getenv(\"MISTRAL_API_KEY\")))\n",
    "\n",
    "# --- 3) Import local modules ---\n",
    "from src.config import MISTRAL_API_KEY\n",
    "from src.pdf_utils import load_all_pdfs_text\n",
    "from src.text_utils import clean_text, chunk_text\n",
    "from src.indexer import build_index, search\n",
    "from src.summarizer import summarize_chunks\n",
    "from src.analyst import analyze_chunks\n",
    "from src.metadata_utils import extract_metadata\n",
    "from src.io_utils import safe_stem\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83260916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded PDF: attention_is_all_you_need.pdf\n",
      "\n",
      "--- First 500 chars of raw text ---\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez‚àó ‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu≈Åukasz Kaise\n"
     ]
    }
   ],
   "source": [
    "# Path to the sample paper\n",
    "pdf_path = project_root / \"data/sample_papers/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Load all PDFs in that folder\n",
    "pairs = load_all_pdfs_text(pdf_path.parent)\n",
    "\n",
    "if not pairs:\n",
    "    raise FileNotFoundError(f\"No PDFs found in {pdf_path.parent.resolve()}\")\n",
    "\n",
    "pdf_path, raw_text = pairs[0]\n",
    "print(\"‚úÖ Loaded PDF:\", pdf_path.name)\n",
    "print(\"\\n--- First 500 chars of raw text ---\\n\")\n",
    "print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76fa2993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata extracted:\n",
      "{\n",
      "  \"title\": \"Attention Is All You Need\",\n",
      "  \"authors\": null,\n",
      "  \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "meta = extract_metadata(pdf_path)\n",
    "\n",
    "print(\"‚úÖ Metadata extracted:\")\n",
    "print(json.dumps(meta, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1250ab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks: 30\n",
      "\n",
      "--- First 2 chunks ---\n",
      "\n",
      "[Chunk 1]\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Goog...\n",
      "\n",
      "[Chunk 2]\n",
      "rench translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "‚àóEqual contri...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "cleaned = clean_text(raw_text)\n",
    "\n",
    "# Chunk text\n",
    "chunks = chunk_text(cleaned, max_chars=1500, overlap=150)\n",
    "\n",
    "print(f\"‚úÖ Total chunks: {len(chunks)}\")\n",
    "print(\"\\n--- First 2 chunks ---\\n\")\n",
    "for i, ch in enumerate(chunks[:2]):\n",
    "    print(f\"[Chunk {i+1}]\\n{ch[:400]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1012c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Top hits:\n",
      "- chunk 30 (score=0.000)\n",
      "n\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two suc \n",
      "\n",
      "- chunk 29 (score=0.000)\n",
      "rent colors represent different heads. Best viewed in color.\n",
      "13\n",
      "\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "< \n",
      "\n",
      "- chunk 28 (score=0.000)\n",
      " Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144 , 2016.\n",
      "[39] Jie  \n",
      "\n",
      "- chunk 27 (score=0.000)\n",
      " Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arX \n",
      "\n",
      "- chunk 26 (score=0.000)\n",
      "ural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "11\n",
      "\n",
      "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn tre \n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = build_index([(f\"chunk {i+1}\", ch) for i, ch in enumerate(chunks)])\n",
    "hits = search(index, \"Summarize contributions and limitations.\", k=5)\n",
    "\n",
    "print(\"‚úÖ Top hits:\")\n",
    "for score, (lbl, txt) in hits:\n",
    "    print(f\"- {lbl} (score={score:.3f})\")\n",
    "    print(txt[:200], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f13478f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary (first 500 chars):\n",
      " - The paper \"Attention Is All You Need\" discusses a new model for neural machine translation that uses self-attention mechanisms.\n",
      "- The self-attention mechanism allows the model to focus on different parts of the input sequence when generating each output word.\n",
      "- The authors provide visualizations of the attention weights for different layers and heads, showing that some heads seem to perform tasks related to sentence structure or anaphora resolution.\n",
      "- One example provided is the sentence \"The \n",
      "\n",
      "---\n",
      "\n",
      "‚úÖ Analysis (first 500 chars):\n",
      " # Analysis: Attention Is All You Need\n",
      "\n",
      "\n",
      "## Methods\n",
      "\n",
      "- Method: Transformer model (as described in \"Attention is All You Need\" by Vaswani et al.)\n",
      "- Architecture: Not explicitly stated, but it's a Transformer model with 6 layers.\n",
      "- Datasets: Not explicitly stated, but the context suggests it could be related to machine translation or language understanding tasks.\n",
      "- Training Setup:\n",
      "  - Optimizer: Not explicitly stated.\n",
      "  - Learning Rate (lr): Not explicitly stated.\n",
      "  - Steps: Not explicitly stated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_chunks = [txt for _s, (_lbl, txt) in hits]\n",
    "\n",
    "summary = summarize_chunks(MISTRAL_API_KEY, \"Attention Is All You Need\", top_chunks)\n",
    "analysis = analyze_chunks(MISTRAL_API_KEY, \"Attention Is All You Need\", top_chunks)\n",
    "\n",
    "print(\"‚úÖ Summary (first 500 chars):\\n\", summary[:500])\n",
    "print(\"\\n---\\n\")\n",
    "print(\"‚úÖ Analysis (first 500 chars):\\n\", analysis[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa328b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata JSON saved to: /home/isa/code/research_gpt_assistant/results/metadata/attention_is_all_you_need_meta.json\n",
      "{\n",
      "  \"file\": \"attention_is_all_you_need.pdf\",\n",
      "  \"title\": \"Attention Is All You Need\",\n",
      "  \"authors\": null,\n",
      "  \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\u2020Work performed while at Google Brain.\\n\\u2021Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023\",\n",
      "  \"query_used\": \"Summarize contributions and limitations.\",\n",
      "  \"outputs\": {\n",
      "    \"summary_md\": \"/home/isa/code/research_gpt_assistant/results/summaries/attention_is_all_you_need_summary.md\",\n",
      "    \"analysis_md\": \"/home/isa/code/research_gpt_assistant/results/analyses/attention_is_all_you_need_analysis.md\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "meta_out = {\n",
    "    \"file\": pdf_path.name,\n",
    "    \"title\": meta.get(\"title\", pdf_path.stem),\n",
    "    \"authors\": meta.get(\"authors\", \"Unknown\"),\n",
    "    \"abstract\": meta.get(\"abstract\"),\n",
    "    \"query_used\": \"Summarize contributions and limitations.\",\n",
    "    \"outputs\": {\n",
    "        \"summary_md\": str(project_root / \"results/summaries\" / f\"{safe_stem(pdf_path)}_summary.md\"),\n",
    "        \"analysis_md\": str(project_root / \"results/analyses\" / f\"{safe_stem(pdf_path)}_analysis.md\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "meta_dir = project_root / \"results/metadata\"\n",
    "meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "meta_path = meta_dir / f\"{safe_stem(pdf_path)}_meta.json\"\n",
    "meta_path.write_text(json.dumps(meta_out, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"‚úÖ Metadata JSON saved to:\", meta_path)\n",
    "print(json.dumps(meta_out, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b047efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch Report Loaded Successfully!\n",
      "\n",
      "             timestamp                           file  \\\n",
      "0  2025-10-11T17:12:07  attention_is_all_you_need.pdf   \n",
      "1  2025-10-11T17:24:45  attention_is_all_you_need.pdf   \n",
      "\n",
      "                                 query_used  \\\n",
      "0  Summarize contributions and limitations.   \n",
      "1  Summarize contributions and limitations.   \n",
      "\n",
      "                                        summary_path  \\\n",
      "0  results/summaries/attention_is_all_you_need_su...   \n",
      "1  results/summaries/attention_is_all_you_need_su...   \n",
      "\n",
      "                                       analysis_path  duration_sec  \n",
      "0  results/analyses/attention_is_all_you_need_ana...          6.54  \n",
      "1  results/analyses/attention_is_all_you_need_ana...          6.59  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/batch_report.csv\") if not Path(\"results/batch_report.csv\").exists() else pd.read_csv(\"results/batch_report.csv\")\n",
    "\n",
    "print(\"‚úÖ Batch Report Loaded Successfully!\\n\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3cc49c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary Insights:\n",
      "Total PDFs processed: 2\n",
      "Average runtime: 6.56 seconds\n",
      "Fastest run: 6.54 seconds\n",
      "Slowest run: 6.59 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Summary Insights:\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "if \"duration_sec\" in df.columns:\n",
    "    print(f\"Total PDFs processed: {len(df)}\")\n",
    "    print(f\"Average runtime: {df['duration_sec'].mean():.2f} seconds\")\n",
    "    print(f\"Fastest run: {df['duration_sec'].min():.2f} seconds\")\n",
    "    print(f\"Slowest run: {df['duration_sec'].max():.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No 'duration_sec' column found ‚Äî check CSV headers below:\")\n",
    "    print(df.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-gpt-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
